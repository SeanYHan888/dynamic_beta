policy_name: meta-llama/Llama-3.2-1B
ref_name: meta-llama/Llama-3.2-1B
precision: bf16

dataset:
  dataset_name: Anthropic/hh-rlhf
  subset: train[:20%]
  val_ratio: 0.1
  seed: 42
  max_len: 512
  
dpo_training:
  epochs: 1
  batch_size: 16
  learning_rate: 5e-6
  log_steps: 10
  warmup_steps: 120
  max_grad_norm: 10
  save_dpo_dir: dpo_model
  save_dir: dynamic_dpo_model

test:
  subset: test
  test_num: 200
  seed: 42
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  batch_size: 4
  dpo_out_dir: eval_outputs/dpo_out.jsonl
  dynamic_dpo_out_dir: eval_outputs/dynamic_dpo_out.jsonl
  ref_out_dir: eval_outputs/ref_out.jsonl

risk_test:
  delta: 0.1
  eplison_0: 0.05
  lambda: 0.05

beta_update:
  beta_0: 0.1
  gamma: 2.0
  alpha: 0.0015
  beta_max: 2.0
  beta_min: 0.01

margin_log:
  log_dir: logs/margins
  dpo_log_dir: logs/dpo_margins
  log_every: 50
  sample_size: 64
  save_npy: true

fsdp:
  enabled: true
  auto_wrap_layers:
    - LlamaDecoderLayer
  limit_all_gathers: true



