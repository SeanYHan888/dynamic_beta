policy_name: meta-llama/Llama-3.2-1B
ref_name: meta-llama/Llama-3.2-1B
precision: bf16
mixed_precision: bf16

dataset:
  dataset_name: Anthropic/hh-rlhf
  subset: train[:20%]
  val_ratio: 0.1
  seed: 42
  max_len: 512
  min_response_tokens: 32
  truncate_prompt: true
  
dpo_training:
  epochs: 1
  beta: 0.1
  batch_size: 16
  learning_rate: 5e-7
  log_steps: 10
  warmup_steps: 20
  max_grad_norm: 10
  save_dpo_dir: dpo_model
  save_dir: dynamic_dpo_model

test:
  subset: test
  test_num: 200
  seed: 42
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  batch_size: 4
  dpo_out_dir: eval_outputs/dpo_out.jsonl
  dynamic_dpo_out_dir: eval_outputs/dynamic_dpo_out.jsonl
  ref_out_dir: eval_outputs/ref_out.jsonl
  load_model_dir: dynamic_dpo_model/hf_pretrained

risk_test:
  delta: 0.1
  eplison_0: 0.1
  lambda: 0.05

beta_update:
  beta_0: 0.1
  gamma: 2.0
  alpha: 0.0015
  beta_max: 2.0
  beta_min: 0.00

margin_log:
  log_dir: logs/margins
  dpo_log_dir: logs/dpo_margins
  log_every: 50
  sample_size: 64
  save_npy: true

debug:
  max_batches: 100
  max_preview_tokens: 512
  print_batches: 3
  log_path: logs/debug_batches.jsonl

fsdp:
  enabled: true
  version: 2
  reshard_after_forward: true
  limit_all_gathers: true
  save_sharded: true





